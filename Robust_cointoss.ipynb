{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute or robust version of the Q-Learning algorithm (in finite spaces)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import numpy as np\n",
    "import copy \n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the q-learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robust_q_learning_v2 import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Robust\" Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    return(a * (y>x) - a * (y<x) - np.abs(a) * (x==y)) # Reward function\n",
    "\n",
    "def P1_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "def p1_0(x,a,y):\n",
    "    return binom.pmf(y,nr_coins,0.5)\n",
    "\n",
    "# Adding some robustness to the model of a \"fair coin\"\n",
    "eps = 0.1\n",
    "def Pp_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5 + eps)\n",
    "def pp_0(x,a,y):\n",
    "    return binom.pmf(y,nr_coins,0.5 + eps)\n",
    "def Pm_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5 - eps)\n",
    "def pm_0(x,a,y):\n",
    "    return binom.pmf(y,nr_coins,0.5 - eps)\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "k_0        = 0    # Initial index of the corresponding MDP, starting with the central proba of 1/2\n",
    "eps_greedy = 0.1  # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 100_000\n",
    "\n",
    "Q_opt_robust_, Visits = robust_q_learning_v2(X, A, r, np.array([P1_0, Pm_0, Pp_0]), np.array([p1_0, pm_0, pp_0]), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the functions that allow us to get the index of an element a (reps. x) in A (resp. X)\n",
    "if np.ndim(A) > 1:\n",
    "    A_list = A\n",
    "else:\n",
    "    A_list = np.array([[a] for a in A])\n",
    "if np.ndim(X) > 1:\n",
    "    X_list = X\n",
    "else:\n",
    "    X_list = np.array([[x] for x in X])\n",
    "\n",
    "def a_index(a):\n",
    "    return np.flatnonzero((a==A_list).all(1))[0]\n",
    "def x_index(x):\n",
    "    return np.flatnonzero((x==X_list).all(1))[0]\n",
    "\n",
    "# Get the result of the Q-Learning algorithm,\n",
    "# Get the optimal results for each x in X\n",
    "def a_opt(x, Q_opt):\n",
    "    return A[np.argmax(Q_opt[x_index(x),:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust_) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study of the sensitivity to the size of the probability range and the number of probabilities considered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initialize our Q with the non-robust algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning import *\n",
    "\n",
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    return(a * (y>x) - a * (y<x) - np.abs(a) * (x==y)) # Reward function\n",
    "\n",
    "def P_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "eps_greedy = 0.1  # Epsilon greedy policy\n",
    "\n",
    "Nr_iter = 100_000\n",
    "Q_opt_nonrobust = q_learning(X, A, r, P_0, alpha, x_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then study the impact of the transition kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  #reinitialize tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS  = [1, 2, 0.5]\n",
    "Nr_p = [2, 5, 10, 25, 50, 100]\n",
    "    \n",
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x, a, y):\n",
    "    return(a * (y > x) - a * (y < x) - np.abs(a) * (x == y)) # Reward function\n",
    "\n",
    "def P1_0(x, a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "def p1_0(x,a,y):\n",
    "    return binom.pmf(y, nr_coins,0.5)\n",
    "\n",
    "\n",
    "# Adding some robustness to the model of a \"fair coin\"\n",
    "eps     = EPS[0] / nr_coins\n",
    "nr_prob = Nr_p[5]\n",
    "P       = [P1_0]\n",
    "p       = [p1_0]\n",
    "for n in range(1, nr_prob//2 + 1):\n",
    "    def Pp(x,a):\n",
    "        return binom.rvs(nr_coins, 0.5 + (eps * n / (nr_prob//2)))\n",
    "    P += [Pp]\n",
    "    def pp(x,a,y):\n",
    "        return binom.pmf(y, nr_coins, 0.5 + (eps * n / (nr_prob//2)))\n",
    "    p += [pp]\n",
    "    def Pm(x,a):\n",
    "        return binom.rvs(nr_coins, 0.5 - (eps * n / (nr_prob//2)))\n",
    "    P += [Pm]\n",
    "    def pm(x,a,y):\n",
    "        return binom.pmf(y, nr_coins, 0.5 - (eps * n / (nr_prob//2)))\n",
    "    p += [pm]\n",
    "print(len(P))     # Verification\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "k_0        = 0\n",
    "eps_greedy = 0.1  # Epsilon greedy policy\n",
    "\n",
    "Nr_iter = 100_000\n",
    "\n",
    "Q_opt_robust__, Visits_ = robust_q_learning_v2(X, A, r, np.array(P), np.array(p), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = Q_opt_nonrobust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the functions that allow us to get the index of an element a (reps. x) in A (resp. X)\n",
    "if np.ndim(A) > 1:\n",
    "    A_list = A\n",
    "else:\n",
    "    A_list = np.array([[a] for a in A])\n",
    "if np.ndim(X) > 1:\n",
    "    X_list = X\n",
    "else:\n",
    "    X_list = np.array([[x] for x in X])\n",
    "\n",
    "def a_index(a):\n",
    "    return np.flatnonzero((a==A_list).all(1))[0]\n",
    "def x_index(x):\n",
    "    return np.flatnonzero((x==X_list).all(1))[0]\n",
    "\n",
    "# Get the result of the Q-Learning algorithm,\n",
    "# Get the optimal results for each x in X\n",
    "def a_opt(x, Q_opt):\n",
    "    return A[np.argmax(Q_opt[x_index(x),:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust__) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(Visits_, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visits_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally comparison with -Julian Sester- Wassersstein uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from julian_sester__Q_learning import *\n",
    "from tqdm import tqdm  #reinitialize tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS  = [1, 2, 0.5]\n",
    "    \n",
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def c(x, y):\n",
    "    return np.abs(x-y)\n",
    "\n",
    "def r(x, a, y):\n",
    "    return(a * (y > x) - a * (y < x) - np.abs(a) * (x == y)) # Reward function\n",
    "\n",
    "def P1_0(x, a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "def p1_0(x,a,y):\n",
    "    return binom.pmf(y, nr_coins,0.5)\n",
    "\n",
    "\n",
    "# Adding some robustness to the model of a \"fair coin\"\n",
    "epsilon = EPS[0]\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "k_0        = 0\n",
    "eps_greedy = 0.1  # Epsilon greedy policy\n",
    "\n",
    "Nr_iter = 100_000\n",
    "\n",
    "Q_opt_robust_js = js_robust_q_learning(X, A, r, c, P1_0, p1_0, epsilon, alpha, x_0, eps_greedy, Nr_iter, q = 1, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "\n",
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust_js) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With other types of probability distribution of the range of probabilities itself"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non_symmetric range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    return(a * (y>x) - a * (y<x) - np.abs(a) * (x==y)) # Reward function\n",
    "\n",
    "def P1_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "def p1_0(x,a,y):\n",
    "    return binom.pmf(y,nr_coins,0.5)\n",
    "\n",
    "# Adding some robustness to the model of a \"fair coin\"\n",
    "def P2_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.6)\n",
    "def p2_0(x,a,y):\n",
    "    return binom.pmf(y,nr_coins,0.6)\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "k_0        = 0    # Initial index of the corresponding MDP, starting with the central proba of 1/2\n",
    "eps_greedy = 0.1  # Epsilon greedy policy\n",
    "\n",
    "Nr_iter = 100_000\n",
    "\n",
    "Q_opt_robust_, Visits = robust_q_learning_v2(X, A, r, np.array([P1_0, P2_0]), np.array([p1_0, p2_0]), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "\n",
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust_) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normally distributed range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    return(a * (y>x) - a * (y<x) - np.abs(a) * (x==y)) # Reward function\n",
    "\n",
    "L = np.random.normal(0.5, 0.05, 25)\n",
    "\n",
    "P = []\n",
    "p = []\n",
    "for p_ in L:\n",
    "    def P_0(x,a):\n",
    "        return binom.rvs(nr_coins, p_)\n",
    "    P += [P_0]\n",
    "    def p_0(x,a,y):\n",
    "        return binom.pmf(y,nr_coins, p_)\n",
    "    p += [p_0]\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "k_0        = 0    # Initial index of the corresponding MDP, starting with the central proba of 1/2\n",
    "eps_greedy = 0.1  # Epsilon greedy policy\n",
    "\n",
    "Nr_iter = 100_000\n",
    "\n",
    "Q_opt_robust_, Visits = robust_q_learning_v2(X, A, r, np.array(P), np.array(p), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "\n",
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust_) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
