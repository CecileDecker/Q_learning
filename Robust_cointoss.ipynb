{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "\n",
    "start = default_timer()\n",
    "\n",
    "# do stuff\n",
    "\n",
    "duration = default_timer() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "#code_block\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal distrib: $[\\mu - 2\\sigma, \\mu + 2\\sigma]$ 95% of proba in, and for $\\mu +/- 3\\sigma$, 99.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute or robust version of the Q-Learning algorithm (in finite spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import numpy as np\n",
    "import copy \n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the q-learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Robust\" Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    return(a * (y>x) - a * (y<x) - np.abs(a) * (x==y)) # Reward function\n",
    "\n",
    "def P1_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "\n",
    "# Adding some robustness to the model of a \"fair coin\"\n",
    "eps = 0.1\n",
    "def Pp_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5 + eps)\n",
    "def Pm_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5 - eps)\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "eps_greedy = 0.1  # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 92782/100000 [04:08<00:17, 413.37it/s]"
     ]
    }
   ],
   "source": [
    "Nr_iter = 100_000\n",
    "\n",
    "Q_opt_robust = robust_q_learning(X, A, r, np.array([P1_0, Pp_0, Pm_0]), alpha, x_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the functions that allow us to get the index of an element a (reps. x) in A (resp. X)\n",
    "if np.ndim(A)>1:\n",
    "    A_list = A\n",
    "else:\n",
    "    A_list = np.array([[a] for a in A])\n",
    "if np.ndim(X)>1:\n",
    "    X_list = X\n",
    "else:\n",
    "    X_list = np.array([[x] for x in X])\n",
    "\n",
    "def a_index(a):\n",
    "    return np.flatnonzero((a==A_list).all(1))[0]\n",
    "def x_index(x):\n",
    "    return np.flatnonzero((x==X_list).all(1))[0]\n",
    "\n",
    "# Get the result of the Q-Learning algorithm,\n",
    "# Get the optimal results for each x in X\n",
    "def a_opt(x, Q_opt):\n",
    "    return A[np.argmax(Q_opt[x_index(x),:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = [1, 2, 0.5]\n",
    "\n",
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    return(a * (y>x) - a * (y<x) - np.abs(a) * (x==y)) # Reward function\n",
    "\n",
    "def P1_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "\n",
    "\n",
    "# Adding some robustness to the model of a \"fair coin\"\n",
    "eps     = EPS[0] / nr_coins\n",
    "nr_prob = 50\n",
    "P       = []\n",
    "for n in range(nr_prob//2 + 1):\n",
    "    def Pp(x,a):\n",
    "        return binom.rvs(nr_coins, 0.5 + (eps * n / (nr_prob//2)))\n",
    "    P += [Pp]\n",
    "    def Pm(x,a):\n",
    "        return binom.rvs(nr_coins, 0.5 - (eps * n / (nr_prob//2)))\n",
    "    P += [Pm]\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "eps_greedy = 0.1  # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 10_000\n",
    "\n",
    "Q_opt_robust = robust_q_learning(X, A, r, np.array(P), alpha, x_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the functions that allow us to get the index of an element a (reps. x) in A (resp. X)\n",
    "if np.ndim(A)>1:\n",
    "    A_list = A\n",
    "else:\n",
    "    A_list = np.array([[a] for a in A])\n",
    "if np.ndim(X)>1:\n",
    "    X_list = X\n",
    "else:\n",
    "    X_list = np.array([[x] for x in X])\n",
    "\n",
    "def a_index(a):\n",
    "    return np.flatnonzero((a==A_list).all(1))[0]\n",
    "def x_index(x):\n",
    "    return np.flatnonzero((x==X_list).all(1))[0]\n",
    "\n",
    "# Get the result of the Q-Learning algorithm,\n",
    "# Get the optimal results for each x in X\n",
    "def a_opt(x, Q_opt):\n",
    "    return A[np.argmax(Q_opt[x_index(x),:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
