{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to model the impact of fishing quotas on a fish population, while taking into account food availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import numpy as np\n",
    "import copy \n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the q-learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robust_q_learning import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First simple settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([(0,0), (0,5), (0,10), (5,0), (5,5), (5,10), (10,0), (10,5), (10,10)]) # States\n",
    "A = np.array([0, 5, 10]) # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    x1, x2 = x\n",
    "    y1, y2 = y\n",
    "    return(a * (x2 - a >= 0))# + (y1 + y2 > 0)) # Reward function\n",
    "\n",
    "def P(x,a):\n",
    "    x1, x2 = x\n",
    "    if a==0:\n",
    "        return (min(x2, 10), min(x1 + x2, 10))\n",
    "    elif a==5:\n",
    "        return (max(x2 - 5, 0), min(x1 + max(x2 - 5, 0), 10))\n",
    "    elif a==10:\n",
    "        return (0, x1)\n",
    "\n",
    "alpha      = 0.95  # Discount Factor\n",
    "x_0        = (5,5) # Initial Value\n",
    "k_0        = 0     # Initial index of the corresponding MDP, starting with the central proba of 1/2\n",
    "eps_greedy = 0.1   # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 100_000\n",
    "\n",
    "Q_opt_robust, Gamma = robust_q_learning(X, A, r, np.array([P]), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the functions that allow us to get the index of an element a (reps. x) in A (resp. X)\n",
    "if np.ndim(A) > 1:\n",
    "    A_list = A\n",
    "else:\n",
    "    A_list = np.array([[a] for a in A])\n",
    "if np.ndim(X) > 1:\n",
    "    X_list = X\n",
    "else:\n",
    "    X_list = np.array([[x] for x in X])\n",
    "\n",
    "def a_index(a):\n",
    "    return np.flatnonzero((a==A_list).all(1))[0]\n",
    "def x_index(x):\n",
    "    return np.flatnonzero((x==X_list).all(1))[0]\n",
    "\n",
    "# Get the result of the Q-Learning algorithm,\n",
    "# Get the optimal results for each x in X\n",
    "def a_opt(x, Q_opt):\n",
    "    return A[np.argmax(Q_opt[x_index(x),:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust) for x in X]]))\n",
    "df.columns = ['(0,0)', '(0,5)', '(0,10)', '(5,0)', '(5,5)', '(5,10)', '(10,0)', '(10,5)', '(10,10)']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Gamma)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
