{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to model the impact of fishing quotas on a fish population, while taking into account food availability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import numpy as np\n",
    "import copy \n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the q-learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robust_q_learning_v3 import *\n",
    "from q_learning_v2 import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First simplified case: one species, two size class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([(0,0), (0,5), (5,0), (5,5)]) # States\n",
    "A = np.array([0, 5]) # Actions\n",
    "\n",
    "c1 = 1 / 5\n",
    "\n",
    "def r(x,a,y):\n",
    "    x1, x2 = x\n",
    "    y1, y2 = y\n",
    "    return(c1 * (x2 - a >= 0) * a - 10 * (x2 - a < 0)) # Reward function\n",
    "\n",
    "rr = 1\n",
    "dr = 0\n",
    "gr = 1\n",
    "npp = 0\n",
    "def P(x,a):\n",
    "    x1, x2 = x\n",
    "    x2_ = max(x2 - a, 0)\n",
    "    y1  = min(rr * x2_ + npp * 5, 5)\n",
    "    y2  = min(x2_ - 5 * dr + gr * x1, 5)\n",
    "    return(y1, y2)\n",
    "    \n",
    "eps1 = 0.75\n",
    "npp2 = 1\n",
    "def P2(x,a):\n",
    "    x1, x2 = x\n",
    "    unif = np.random.uniform(0)\n",
    "    x2_ = max(x2 - a, 0)\n",
    "    y1  = min(rr * x2_ + ((unif > eps1) * npp + (unif <= eps1) * npp2) * 5 , 5)\n",
    "    y2  = min(x2_ - 5 * dr + gr * x1, 5)\n",
    "    return(y1, y2)\n",
    "\n",
    "alpha      = 0.95  # Discount Factor\n",
    "x_0        = (5,5) # Initial Value\n",
    "eps_greedy = 0.1   # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the functions that allow us to get the index of an element a (reps. x) in A (resp. X)\n",
    "if np.ndim(A) > 1:\n",
    "    A_list = A\n",
    "else:\n",
    "    A_list = np.array([[a] for a in A])\n",
    "if np.ndim(X) > 1:\n",
    "    X_list = X\n",
    "else:\n",
    "    X_list = np.array([[x] for x in X])\n",
    "\n",
    "def a_index(a):\n",
    "    return np.flatnonzero((a==A_list).all(1))[0]\n",
    "def x_index(x):\n",
    "    return np.flatnonzero((x==X_list).all(1))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First non-robust runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 1_000_000\n",
    "\n",
    "Q_0_, V = q_learning(X, A, r, P, alpha, x_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result of the Q-Learning algorithm,\n",
    "# Get the optimal results for each x in X\n",
    "def a_opt(x, Q_opt):\n",
    "    return A[np.argmax(Q_opt[x_index(x),:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[a_opt(x, Q_0_) for x in X]]))\n",
    "df.columns = ['(0,0)', '(0,5)', '(5,0)', '(5,5)']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the accuracy of the result with the other function, and adding uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([(0,0), (0,5), (5,0), (5,5)]) # States\n",
    "A = np.array([0, 5]) # Actions\n",
    "\n",
    "c1 = 1 / 5\n",
    "\n",
    "def r(x,a,y):\n",
    "    x1, x2 = x\n",
    "    y1, y2 = y\n",
    "    return(c1 * (x2 - a >= 0) * a - (x2 - a < 0)) # Reward function\n",
    "\n",
    "rr = 1\n",
    "dr = 0\n",
    "gr = 1\n",
    "npp = 0\n",
    "def P1(x,a):\n",
    "    x1, x2 = x\n",
    "    x2_ = max(x2 - a, 0)\n",
    "    y1  = min(rr * x2_ + npp * 5, 5)\n",
    "    y2  = min(x2_ - 5 * dr + gr * x1, 5)\n",
    "    #if a==0:\n",
    "    #    return (x2 , min(x1 + x2, 5))\n",
    "    #elif a==5:\n",
    "    #    return (0, x1)\n",
    "    return(y1, y2)\n",
    "    \n",
    "eps1 = 1\n",
    "npp2 = 1\n",
    "def P2(x,a):\n",
    "    x1, x2 = x\n",
    "    unif = np.random.uniform(0)\n",
    "    x2_ = max(x2 - a, 0)\n",
    "    y1  = min(rr * x2_ + ((unif > eps1) * npp + (unif <= eps1) * npp2) * 5 , 5)\n",
    "    y2  = min(x2_ - 5 * dr + gr * x1, 5)\n",
    "    #if a==0:\n",
    "    #    unif      = np.random.uniform(0)\n",
    "    #    return (unif > eps1) * (x2 , min(x1 + x2, 5)) + (unif <= eps1) * (5 , min(x1 + x2, 5))\n",
    "    #elif a==5:\n",
    "    #    unif      = np.random.uniform(0)\n",
    "    #    return (unif > eps2) * (0, x1) + (unif <= eps2) * (5, x1)\n",
    "    return(y1, y2)\n",
    "\n",
    "    \n",
    "# CREATE THE PROBABILITY MEASURE OUT OF THE RANDOM VARIABLE\n",
    "nr = 1_000\n",
    "p1_ = np.zeros([len(X), len(A), len(X)])\n",
    "p2_ = np.zeros([len(X), len(A), len(X)])\n",
    "for n in range(nr):\n",
    "    for x in X:\n",
    "        for a in A:\n",
    "            y1 = P1(x,a)\n",
    "            x_1 = x_index(y1)\n",
    "            p1_[x_index(x), a_index(a), x_1] += 1\n",
    "            y2 = P2(x,a)\n",
    "            x_2 = x_index(y2)\n",
    "            p2_[x_index(x), a_index(a), x_2] += 1\n",
    "p1_ = p1_/nr\n",
    "p2_ = p2_/nr\n",
    "def p1(x,a,y):\n",
    "    return(p1_[x_index(x), a_index(a), x_index(y)])\n",
    "def p2(x,a,y):\n",
    "    return(p2_[x_index(x), a_index(a), x_index(y)])\n",
    "\n",
    "alpha      = 0.95  # Discount Factor\n",
    "x_0        = (5,5) # Initial Value\n",
    "k_0        = 0     # Initial index of the corresponding MDP\n",
    "eps_greedy = 0.1   # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_opt_robust, V = robust_q_learning_v2(X, A, r, np.array([P1, P2]), np.array([p1, p2]), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust) for x in X]]))\n",
    "df.columns = ['(0,0)', '(0,5)', '(5,0)', '(5,5)']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now put more possible states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import numpy as np\n",
    "import copy \n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robust_q_learning_v3 import *\n",
    "from q_learning_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([(0,0), (0,5), (0,10), (5,0), (5,5), (5,10), (10,0), (10,5), (10,10)]) # States\n",
    "A = np.array([0, 5, 10]) # Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the functions that allow us to get the index of an element a (reps. x) in A (resp. X)\n",
    "if np.ndim(A) > 1:\n",
    "    A_list = A\n",
    "else:\n",
    "    A_list = np.array([[a] for a in A])\n",
    "if np.ndim(X) > 1:\n",
    "    X_list = X\n",
    "else:\n",
    "    X_list = np.array([[x] for x in X])\n",
    "\n",
    "def a_index(a):\n",
    "    return np.flatnonzero((a==A_list).all(1))[0]\n",
    "def x_index(x):\n",
    "    return np.flatnonzero((x==X_list).all(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 1 / 10\n",
    "\n",
    "def r(x,a,y):\n",
    "    x1, x2 = x\n",
    "    y1, y2 = y\n",
    "    return(c1 * (x2 - a >= 0) * a - 10 * (x2 - a < 0)) # Reward function\n",
    "\n",
    "rr = 1\n",
    "dr = 0\n",
    "gr = 1\n",
    "npp = 0\n",
    "def P(x,a):\n",
    "    x1, x2 = x\n",
    "    x2_ = max(x2 - a, 0)\n",
    "    y1  = min(rr * x2_ + npp * 5, 10)\n",
    "    y2  = min(x2_ - 5 * dr + gr * x1, 10)\n",
    "    #if a==0:\n",
    "    #    return (x2 , min(x1 + x2, 10))\n",
    "    #elif a==5:\n",
    "    #    return (max(x2 - 5, 0), min(x1 + max(x2 - 5, 0), 10))\n",
    "    #elif a==10:\n",
    "    #    return (0, x1)\n",
    "    return(y1, y2)\n",
    "\n",
    "# CREATE THE PROBABILITY MEASURE OUT OF THE RANDOM VARIABLE\n",
    "nr = 1_000\n",
    "p_ = np.zeros([len(X), len(A), len(X)])\n",
    "for n in range(nr):\n",
    "    for x in X:\n",
    "        for a in A:\n",
    "            y = P(x,a)\n",
    "            x_ = x_index(y)\n",
    "            p_[x_index(x), a_index(a), x_] += 1\n",
    "p_ = p_/nr\n",
    "def p(x,a,y):\n",
    "    return(p_[x_index(x), a_index(a), x_index(y)])\n",
    "\n",
    "alpha      = 0.95  # Discount Factor\n",
    "x_0        = (10,10) # Initial Value\n",
    "k_0        = 0     # Initial index of the corresponding MDP, starting with the central proba of 1/2\n",
    "eps_greedy = 0.1   # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 1_000_000\n",
    "\n",
    "Q_0_, V = q_learning(X, A, r, P, alpha, x_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result of the Q-Learning algorithm,\n",
    "# Get the optimal results for each x in X\n",
    "def a_opt(x, Q_opt):\n",
    "    return A[np.argmax(Q_opt[x_index(x),:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[a_opt(x, Q_0_) for x in X]]))\n",
    "df.columns = ['(0,0)', '(0,5)', '(0,10)', '(5,0)', '(5,5)', '(5,10)', '(10,0)', '(10,5)', '(10,10)']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_opt_robust, V = robust_q_learning_v2(X, A, r, np.array([P]), np.array([p]), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust) for x in X]]))\n",
    "df.columns = ['(0,0)', '(0,5)', '(0,10)', '(5,0)', '(5,5)', '(5,10)', '(10,0)', '(10,5)', '(10,10)']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new transition kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 1 / 10\n",
    "\n",
    "def r(x,a,y):\n",
    "    x1, x2 = x\n",
    "    y1, y2 = y\n",
    "    return(c1 * (x2 - a >= 0) * a - 10 * (x2 - a < 0)) # Reward function\n",
    "\n",
    "rr = 1\n",
    "dr = 0\n",
    "gr = 1\n",
    "npp = 0\n",
    "def P1(x,a):\n",
    "    x1, x2 = x\n",
    "    x2_ = max(x2 - a, 0)\n",
    "    y1  = min(rr * x2_ + npp * 5, 10)\n",
    "    y2  = min(x2_ - 5 * dr + gr * x1, 10)\n",
    "    return(y1, y2)\n",
    "\n",
    "eps1 = 1\n",
    "npp2 = 1\n",
    "def P2(x,a):\n",
    "    x1, x2 = x\n",
    "    unif = np.random.uniform(0)\n",
    "    x2_  = max(x2 - a, 0)\n",
    "    y1   = min(rr * x2_ + ((unif > eps1) * npp + (unif <= eps1) * npp2) * 5 , 10)\n",
    "    y2   = min(x2_ - 5 * dr + gr * x1, 10)\n",
    "    return(y1, y2)\n",
    "\n",
    "eps3 = 0.5\n",
    "dr3  = 1\n",
    "rr3   = 2\n",
    "def P3(x,a):\n",
    "    x1, x2 = x\n",
    "    unif = np.random.uniform(0)\n",
    "    x2_  = max(x2 - a, 0)\n",
    "    y1   = min(rr3 * x2_ + npp * 5 , 10)\n",
    "    x2__ = max(x2_ - ((unif > eps3) * dr + (unif <= eps3) * dr3) * 5, 0)\n",
    "    y2   = min(x2__ + gr * x1, 10)\n",
    "    return(y1, y2)\n",
    "\n",
    "    \n",
    "# CREATE THE PROBABILITY MEASURE OUT OF THE RANDOM VARIABLE\n",
    "nr = 1_000\n",
    "p1_ = np.zeros([len(X), len(A), len(X)])\n",
    "p2_ = np.zeros([len(X), len(A), len(X)])\n",
    "p3_ = np.zeros([len(X), len(A), len(X)])\n",
    "for n in range(nr):\n",
    "    for x in X:\n",
    "        for a in A:\n",
    "            y1 = P1(x,a)\n",
    "            x_1 = x_index(y1)\n",
    "            p1_[x_index(x), a_index(a), x_1] += 1\n",
    "            y2 = P2(x,a)\n",
    "            x_2 = x_index(y2)\n",
    "            p2_[x_index(x), a_index(a), x_2] += 1\n",
    "            y3 = P3(x,a)\n",
    "            x_3 = x_index(y3)\n",
    "            p3_[x_index(x), a_index(a), x_3] += 1\n",
    "p1_ = p1_/nr\n",
    "p2_ = p2_/nr\n",
    "p3_ = p3_/nr\n",
    "def p1(x,a,y):\n",
    "    return(p1_[x_index(x), a_index(a), x_index(y)])\n",
    "def p2(x,a,y):\n",
    "    return(p2_[x_index(x), a_index(a), x_index(y)])\n",
    "def p3(x,a,y):\n",
    "    return(p3_[x_index(x), a_index(a), x_index(y)])\n",
    "\n",
    "alpha      = 0.95  # Discount Factor\n",
    "x_0        = (10,10) # Initial Value\n",
    "k_0        = 0     # Initial index of the corresponding MDP, starting with the central proba of 1/2\n",
    "eps_greedy = 0.1   # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 1_000_000\n",
    "Q_0_, V = q_learning(X, A, r, P2, alpha, x_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "df = pd.DataFrame(np.array([[a_opt(x, Q_0_) for x in X]]))\n",
    "df.columns = ['(0,0)', '(0,5)', '(0,10)', '(5,0)', '(5,5)', '(5,10)', '(10,0)', '(10,5)', '(10,10)']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 1_000_000\n",
    "Q_opt_robust, V = robust_q_learning_v2(X, A, r, np.array([P1, P2, P3]), np.array([p1, p2, p3]), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust) for x in X]]))\n",
    "df.columns = ['(0,0)', '(0,5)', '(0,10)', '(5,0)', '(5,5)', '(5,10)', '(10,0)', '(10,5)', '(10,10)']\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets add a new species to simulate the willing to keep a certain biodiversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import numpy as np\n",
    "import copy \n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robust_q_learning_v3 import *\n",
    "from q_learning_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_s1 = np.array([0, 5]) # Actions\n",
    "X    = []\n",
    "for x1 in A_s1:\n",
    "    for x2 in A_s1:\n",
    "        X += [(x1, x2)]\n",
    "X_s1 = np.array(X) # States\n",
    "\n",
    "A_s2 = np.array([0, 5, 10]) # Actions\n",
    "X    = []\n",
    "for x1 in A_s2:\n",
    "    for x2 in A_s2:\n",
    "        X += [(x1, x2)]\n",
    "X_s2 = np.array(X) # States\n",
    "\n",
    "A = []\n",
    "for a1 in A_s1:\n",
    "    for a2 in A_s2:\n",
    "        A += [(a1, a2)]\n",
    "A = np.array(A) # Actions\n",
    "X = []\n",
    "for x1 in X_s1:\n",
    "    for x2 in X_s2:\n",
    "        X += [(x1, x2)]\n",
    "X = np.array(X) # States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the functions that allow us to get the index of an element a (reps. x) in A (resp. X)\n",
    "if np.ndim(A) > 1:\n",
    "    A_list = A\n",
    "else:\n",
    "    A_list = np.array([[a] for a in A])\n",
    "if np.ndim(X) > 1:\n",
    "    X_list = X\n",
    "else:\n",
    "    X_list = np.array([[x] for x in X])\n",
    "\n",
    "def a_index(a):\n",
    "    return np.flatnonzero((a==A_list).all(1))[0]\n",
    "def x_index(x):\n",
    "    return np.flatnonzero((x==X_list).all(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 2 / 5 \n",
    "c2 = 1 / 10 \n",
    "psy  = 1\n",
    "\n",
    "def r(x,a,y):\n",
    "    x_s1, x_s2   = x\n",
    "    y_s1, y_s2   = y\n",
    "    x1_s1, x2_s1 = x_s1\n",
    "    x1_s2, x2_s2 = x_s2\n",
    "    y1_s1, y2_s1 = y_s1\n",
    "    y1_s2, y2_s2 = y_s2\n",
    "    a_s1, a_s2   = a\n",
    "\n",
    "    food_s1      = c1 * (x2_s1 - a_s1 >= 0) * a_s1 - 10 * (x2_s1 - a_s1 < 0)  #Optimization term, regarding the food that the species 1 can give us\n",
    "    food_s2      = c2 * (x2_s2 - a_s2 >= 0) * a_s2 - 10 * (x2_s2 - a_s2 < 0)  #Optimization term, regarding the food that the species 2 can give us\n",
    "    biod         = psy  * (y1_s1 + y2_s1 > 0) * (y1_s2 + y2_s2 > 0)           #Optimization term, regarding the biodiversity we want to keep\n",
    "    return(food_s1 + food_s2 + biod) # Reward function\n",
    "\n",
    "rr = 1\n",
    "dr = 0\n",
    "gr = 1\n",
    "npp = 0\n",
    "def P(x,a):\n",
    "    x_s1, x_s2   = x\n",
    "    x1_s1, x2_s1 = x_s1\n",
    "    x1_s2, x2_s2 = x_s2\n",
    "    a_s1, a_s2   = a \n",
    "    \n",
    "    x2_s1_ = max(x2_s1 - a_s1, 0)\n",
    "    y1_s1  = min(rr * x2_s1_ + npp * 5, 5)\n",
    "    y2_s1  = min(x2_s1_ - 5 * dr + gr * x1_s1, 5)\n",
    "    \n",
    "    x2_s2_ = max(x2_s2 - a_s2, 0)\n",
    "    y1_s2  = min(rr * x2_s2_ + npp * 5, 10)\n",
    "    y2_s2  = min(x2_s2_ - 5 * dr + gr * x1_s2, 10)\n",
    "    \n",
    "    return ((y1_s1, y2_s1), (y1_s2, y2_s2))\n",
    "\n",
    "# CREATE THE PROBABILITY MEASURE OUT OF THE RANDOM VARIABLE\n",
    "nr = 1_000\n",
    "p_ = np.zeros([len(X), len(A), len(X)])\n",
    "for n in range(nr):\n",
    "    for x in X:\n",
    "        for a in A:\n",
    "            y = P(x,a)\n",
    "            x_ = x_index(y)\n",
    "            p_[x_index(x), a_index(a), x_] += 1\n",
    "p_ = p_/nr\n",
    "#print(p_)\n",
    "def p(x,a,y):\n",
    "    return(p_[x_index(x), a_index(a), x_index(y)])\n",
    "\n",
    "alpha      = 0.95  # Discount Factor\n",
    "x_0        = ((5, 5), (10, 10)) # Initial Value\n",
    "k_0        = 0     # Initial index of the corresponding MDP, starting with the central proba of 1/2\n",
    "eps_greedy = 0.1   # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 10_000_000\n",
    "\n",
    "Q_0_, V = q_learning(X, A, r, P, alpha, x_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result of the Q-Learning algorithm,\n",
    "# Get the optimal results for each x in X\n",
    "def a_opt(x, Q_opt):\n",
    "    return A[np.argmax(Q_opt[x_index(x),:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = np.array([[a_opt(x, Q_0_) for x in X]])\n",
    "Result = Result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Result.T)\n",
    "df.columns = [str(x) for x in X]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_bio1_nonrobust.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_opt_robust, V = robust_q_learning_v2(X, A, r, np.array([P]), np.array([p]), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_r = np.array([[a_opt(x, Q_opt_robust) for x in X]])\n",
    "Result_r = Result_r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Result_r.T)\n",
    "df.columns = [str(x) for x in X]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_bio1_robust.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 100_000_000\n",
    "\n",
    "Q_0_, V = q_learning(X, A, r, P, alpha, x_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "\n",
    "Result = np.array([[a_opt(x, Q_0_) for x in X]])\n",
    "Result = Result[0]\n",
    "\n",
    "df = pd.DataFrame(Result.T)\n",
    "df.columns = [str(x) for x in X]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_bio1_nonrobust_moreit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
